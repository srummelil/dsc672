{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import warnings\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from pandas import Series\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    " \n",
    "production=pd.read_pickle(\"./processed/production_all_dates_and_variables.pkl\")\n",
    "keep_these=['Wind_KWH',\n",
    "            'Solar_KWH',\n",
    "            'Wind_Speed_AT_WINDFARM',\n",
    "            'Solar_Elevation',\n",
    "            'Cloud_Cover_Fraction',\n",
    "            'Dew_Point',\n",
    "            'Humidity_Fraction',\n",
    "            'Precipitation',\n",
    "            'Pressure',\n",
    "            'Temperature',\n",
    "            'Visibility',\n",
    "            'Wind_Speed_AT_SOLARRAY']\n",
    "production=production[keep_these]\n",
    "#fill the date and time gaps\n",
    "production = production.resample(\"60min\").asfreq()\n",
    "production.to_pickle(\"./processed/production_full_date_range_pre_impute.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Solar_KWH  Cloud_Cover_Fraction  Dew_Point  Humidity_Fraction  \\\n",
      "Date                                                                            \n",
      "2010-01-04   20084.160000              1.000000  -8.300000           0.698400   \n",
      "2010-01-05  111402.100000              1.000000  -7.000000           0.789400   \n",
      "2010-01-06  163901.380000              1.000000  -6.950000           0.768500   \n",
      "2010-01-07   95813.460000              0.985714  -6.442857           0.865729   \n",
      "2010-01-08  116167.524153              0.987500 -11.062500           0.723737   \n",
      "\n",
      "            Precipitation    Pressure  Temperature  Visibility  \\\n",
      "Date                                                             \n",
      "2010-01-04       0.000000  992.050049    -3.600000   16.093000   \n",
      "2010-01-05       0.000000  992.112488    -3.875000   15.087251   \n",
      "2010-01-06       0.000000  993.799988    -3.475000   16.093000   \n",
      "2010-01-07       0.071429  991.649963    -4.528572   11.954858   \n",
      "2010-01-08       0.000000  996.012512    -6.937500   15.288500   \n",
      "\n",
      "            Wind_Speed_AT_SOLARRAY  \n",
      "Date                                \n",
      "2010-01-04                4.650000  \n",
      "2010-01-05                4.250000  \n",
      "2010-01-06                3.475000  \n",
      "2010-01-07                1.271428  \n",
      "2010-01-08                5.212500  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implement an imputation scheme for each variable.\n",
    "\n",
    "\n",
    "Dependent Variables - no imputation\n",
    "solar - if nighttime, zero\n",
    "wind - ???\n",
    "=========================================\n",
    "'Wind_KWH',\n",
    "'Solar_KWH',\n",
    "\n",
    "Dependent Variables - impute as described\n",
    "CHECK SPARSITY OF VALUES PRIOR TO IMPUTING\n",
    "=========================================\n",
    "'Wind_Speed_AT_WINDFARM', - high variance, \n",
    "'Solar_Elevation', # Removing, as there is no good way to aggregate this by day, nor is it necessarily relevant except\n",
    "                     as a performance measure of the solar array, which is not under analysis.\n",
    "'Cloud_Cover_Fraction', - Average\n",
    "'Dew_Point', - Average\n",
    "'Humidity_Fraction', - Average\n",
    "'Precipitation', - Average\n",
    "'Pressure', - Average\n",
    "'Temperature', - Average\n",
    "'Visibility', - Average\n",
    "'Wind_Speed_AT_SOLARRAY' - Average\n",
    "\n",
    "Note: We should do inter-day analysis as well, if possible.\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "# Create dataset for seasonality of solar KWH by day.\n",
    "\n",
    "solar_prod=production['Solar_KWH']\n",
    "solar_prod_by_day = solar_prod.resample(\"D\").sum().to_frame('Solar_KWH')\n",
    "solar_prod_by_day.to_pickle(\"./processed/solar_prod_by_day.pkl\")\n",
    "\n",
    "independent_vars=['Cloud_Cover_Fraction',\n",
    "                  'Dew_Point',\n",
    "                  'Humidity_Fraction',\n",
    "                  'Precipitation',\n",
    "                  'Pressure',\n",
    "                  'Temperature',\n",
    "                  'Visibility',\n",
    "                  'Wind_Speed_AT_SOLARRAY']\n",
    "\n",
    "indep_vars=production[independent_vars]\n",
    "for field in independent_vars:\n",
    "    # consumption_master = consumption_master.join(calendar, how='outer', sort=True)\n",
    "    # For each field, aggregate by day, then join to solar_prod\n",
    "    tmp=pd.DataFrame(data=indep_vars[field])\n",
    "    tmp.dropna(axis=0, inplace=True)\n",
    "    tmp=tmp.loc[tmp[field]!='nan'] # How we got text 'nan' in here, I do not know, but remove them.\n",
    "    tmp=pd.to_numeric(tmp[field], downcast='float')\n",
    "    # Averaging the remaining values by day should yield a useful metric for modeling, even if\n",
    "    # it would be meaningless from a real weather-reporting context. \n",
    "    # We are imposing the same limitation on all variables, so should still be representative.\n",
    "    tmp=tmp.resample(\"D\").mean().to_frame(field)\n",
    "    #print(tmp.head())\n",
    "    solar_prod_by_day=solar_prod_by_day.join(tmp, how='left', sort=True)\n",
    "    \n",
    "#print(solar_prod_by_day.head())\n",
    "\n",
    "\n",
    "# Now, we need to interpolate missing values.\n",
    "\n",
    "def interpolate_gaps(values, limit=None):\n",
    "    \"\"\"\n",
    "    Fill gaps using linear interpolation, optionally only fill gaps up to a\n",
    "    size of `limit`, courtesy of StackOVerflow:\n",
    "    https://stackoverflow.com/questions/36455083/working-with-nan-values-in-matplotlib\n",
    "    \"\"\"\n",
    "    values = np.asarray(values)\n",
    "    i = np.arange(values.size)\n",
    "    valid = np.isfinite(values)\n",
    "    filled = np.interp(i, i[valid], values[valid])\n",
    "\n",
    "    if limit is not None:\n",
    "        invalid = ~valid\n",
    "        for n in range(1, limit+1):\n",
    "            invalid[:-n] &= invalid[n:]\n",
    "        filled[invalid] = np.nan\n",
    "    return filled\n",
    "\n",
    "\n",
    "columns={}\n",
    "dates=solar_prod_by_day.index.values\n",
    "columns['Date']=dates\n",
    "for field in list(solar_prod_by_day):\n",
    "    raw=solar_prod_by_day[field].values\n",
    "    filled = interpolate_gaps(raw, limit=2)\n",
    "    columns[field]=filled\n",
    "\n",
    "solar_prod_by_day_fnl=pd.DataFrame.from_dict(columns)\n",
    "solar_prod_by_day_fnl.set_index('Date', inplace=True)\n",
    "solar_prod_by_day_fnl.to_pickle(\"./processed/solar_prod_by_day_w_interpolation.pkl\")\n",
    "print(solar_prod_by_day_fnl.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Test: [2010] Train: [2011, 2012, 2013]\n",
      "ARIMA(0, 0, 0)x(0, 0, 0, 360) - AIC:33039.11303362222 - best AIC: 33039.11303362222 - Iteration: 2\n",
      "ARIMA(0, 0, 0)x(0, 0, 1, 360) - AIC:263711263.75175545 - best AIC: 33039.11303362222 - Iteration: 3\n",
      "Exception: maxlag should be < nobs ARIMA(0, 0, 0)x(0, 0, 2, 360) - Iteration: 3\n",
      "ARIMA(0, 0, 0)x(0, 1, 0, 360) - AIC:21514.065046649273 - best AIC: 21514.065046649273 - Iteration: 4\n",
      "Exception: maxlag should be < nobs ARIMA(0, 0, 0)x(0, 1, 1, 360) - Iteration: 4\n",
      "Exception: maxlag should be < nobs ARIMA(0, 0, 0)x(0, 1, 2, 360) - Iteration: 4\n",
      "Exception:  ARIMA(0, 0, 0)x(0, 2, 0, 360) - Iteration: 4\n",
      "Exception: maxlag should be < nobs ARIMA(0, 0, 0)x(0, 2, 1, 360) - Iteration: 4\n",
      "Exception: maxlag should be < nobs ARIMA(0, 0, 0)x(0, 2, 2, 360) - Iteration: 4\n",
      "ARIMA(0, 0, 0)x(1, 0, 0, 360) - AIC:21403.028964338708 - best AIC: 21403.028964338708 - Iteration: 5\n",
      "ARIMA(0, 0, 0)x(1, 0, 1, 360) - AIC:263708104.22930986 - best AIC: 21403.028964338708 - Iteration: 6\n",
      "Exception: maxlag should be < nobs ARIMA(0, 0, 0)x(1, 0, 2, 360) - Iteration: 6\n",
      "Exception:  ARIMA(0, 0, 0)x(1, 1, 0, 360) - Iteration: 6\n",
      "Exception: maxlag should be < nobs ARIMA(0, 0, 0)x(1, 1, 1, 360) - Iteration: 6\n",
      "Exception: maxlag should be < nobs ARIMA(0, 0, 0)x(1, 1, 2, 360) - Iteration: 6\n"
     ]
    }
   ],
   "source": [
    "# From: \n",
    "# https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3\n",
    "# Significantly adapted, but the iterative approach and some code is a direct-use.\n",
    "\n",
    "def find_best_model(train_years, test_years, df, exogenous_variables, endogenous_variable):\n",
    "    train_data=df[endogenous_variable].loc[(df.index.year.isin(train_years))]\n",
    "    exog=df[exogenous_variables].loc[(df.index.year.isin(train_years))].values\n",
    "    train=train_data.values\n",
    "\n",
    "    test_data=df[endogenous_variable].loc[(df.index.year.isin(test_years))]\n",
    "    test=test_data.values\n",
    "\n",
    "    # Define the p, d and q parameters to take any value between 0 and 2\n",
    "    p = d = q = range(0, 3)\n",
    "\n",
    "    # Generate all different combinations of p, q and q triplets\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "    # Generate all different combinations of seasonal p, q and q triplets\n",
    "    # Since we are daily data, m=360, suggesting a yearly cycle.\n",
    "    seasonal_pdq = [(x[0], x[1], x[2], 360) for x in list(itertools.product(p, d, q))]\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "    count=1\n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            try:\n",
    "                mod = sm.tsa.statespace.SARIMAX(train,\n",
    "                                                #exog=exog,\n",
    "                                                order=param,\n",
    "                                                seasonal_order=param_seasonal,\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False)\n",
    "\n",
    "                results = mod.fit()\n",
    "                if count==1:\n",
    "                    tmp_best=results.aic\n",
    "                    count+=1\n",
    "                else:\n",
    "                    if tmp_best > results.aic:\n",
    "                        tmp_best=results.aic\n",
    "                        tmp_best_param=param\n",
    "                        tmp_best_seasonal=param_seasonal\n",
    "                        count+=1\n",
    "                    else:\n",
    "                        count+=1\n",
    "                    \n",
    "                print('ARIMA{}x{} - AIC:{} - best AIC: {} - Iteration: {}'.format(param,\n",
    "                                                                                    param_seasonal,\n",
    "                                                                                    results.aic,\n",
    "                                                                                    tmp_best,\n",
    "                                                                                    count))\n",
    "            \n",
    "            except Exception as e:\n",
    "                print('Exception: {} ARIMA{}x{} - Iteration: {}'.format(e,\n",
    "                                                                          param,\n",
    "                                                                          param_seasonal,\n",
    "                                                                          count))\n",
    "                continue\n",
    "                \n",
    "    return(tmp_best, tmp_best_param, tmp_best_seasonal)\n",
    "\n",
    "\n",
    "# Lets split the data into 3 years train, one test, across years. Assumption: That any long-\n",
    "# term weather trend will not manifest materially within a three year timeframe.\n",
    "\n",
    "years=[2010, 2011, 2012, 2013]\n",
    "\n",
    "# Keep: Visibility, Temperature, and Pressure - Pressure has NaN, dropping it.\n",
    "exogenous_variables=['Visibility', 'Temperature']\n",
    "\n",
    "#for scenario in scenarios:\n",
    "for iteration in range(0, len(years)):\n",
    "    list_len=len(years)\n",
    "    train_years=years[1:len(years)]\n",
    "    test_year=[]\n",
    "    test_year.append(years[0])\n",
    "    print(\"Iteration: {} Test: {} Train: {}\".format(iteration, test_year, train_years))\n",
    "    tmp_best, tmp_best_param, tmp_best_seasonal = find_best_model(train_years,\n",
    "                                                                  test_year,\n",
    "                                                                  solar_prod_by_day_fnl,\n",
    "                                                                  exogenous_variables,\n",
    "                                                                  'Solar_KWH')\n",
    "\n",
    "    print(\"Best param: {} Best Seasonal Param: {} best AIC: {}\".format(tmp_best_param,\n",
    "                                                                       tmp_best_seasonal,\n",
    "                                                                       tmp_best))\n",
    "    years.append(years.pop(0))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(train,\n",
    "                                order=tmp_best_param,\n",
    "                                seasonal_order=tmp_best_seasonal,\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()\n",
    "#imgname=\"SARIMA_train_{}_test_{}_param_{}_seasonal_{}\".format(2010, 2013, tmp_best_param, tmp_best_seasonal)\n",
    "#results.plot_diagnostics(figsize=(15, 12)).savefig(\"./Images/\" + imgname + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(solar_prod_by_day_fnl.corr())\n",
    "import seaborn as sns\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = solar_prod_by_day_fnl.corr()\n",
    "sns.heatmap(corr,\n",
    "            mask=np.zeros_like(corr,\n",
    "                               dtype=np.bool),\n",
    "            cmap=sns.diverging_palette(220,\n",
    "                                       10,\n",
    "                                       as_cmap=True),\n",
    "            square=True, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Visibility, Temperature, and Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Our primary concern is to ensure that the residuals of our model are uncorrelated and \n",
    "normally distributed with zero-mean. If the seasonal ARIMA model does not satisfy these properties, \n",
    "it is a good indication that it can be further improved.\n",
    "\n",
    "In this case, our model diagnostics suggests that the model residuals are roughly normally \n",
    "distributed based on the following:\n",
    "\n",
    "In the top right plot, we see that the red KDE line does follows the N(0,1) line, which \n",
    "is the standard notation for a normal distribution with mean 0 and standard deviation of 1), except for a \n",
    "spike between 0 and 1. This is a good indication that the residuals are roughly normally distributed.\n",
    "\n",
    "The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots)\n",
    "follows the linear trend of the samples taken from a standard normal distribution with N(0, 1).\n",
    "This is a decent indication that the residuals are normally distributed.\n",
    "\n",
    "The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise.\n",
    "\n",
    "This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right,\n",
    "which shows that the time series residuals have low correlation with lagged versions of itself.\n",
    "Those observations lead us to conclude that our model produces a satisfactory fit that could help us\n",
    "understand our time series data and forecast future values.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
